meta:
  sos_version: v0.0.1
  system_name: sos-ollama_webui
  system_version: v0.0.2
  system_internal: internal
  system_description: Open WebUI is an extensible, feature-rich, and user-friendly self-hosted WebUI designed to operate entirely offline. It supports various LLM runners, including Ollama and OpenAI-compatible APIs. For more information, be sure to check out our Open WebUI Documentation.
  profile: default

namespace:
  model_list: null
  ollama_data: null

  frontend_image: ghcr.io/open-webui/open-webui:main

  runtime:
    compose_file: docker-compose.yaml
    host: 0.0.0.0
    port: "3000"

service:
  local_data: True

  ollama: True

action:
  sos_help:
    label: print help
    tool: terminal.print_object
    params:
      horizontal_rule: True
      obj:
        - SOS-Ollama_WebUI: ${meta.system_description}
        - Install:
          - sos-toolkit setup
          - sos-toolkit install
        - Run:
          - sos-toolkit up
        - Stop:
          - sos-toolkit down
        - Config:
          - Change Ollama Model Directory: sos-toolkit config --target ollama_data

  sos_config:
    ollama_data:
      tool: terminal.input_prompt
      params:
        prompt: "Set Ollama Data Directory: "
      result_map:
        - label: set ollama_data
          result: namespace.ollama_data
          data: input
      callbacks:
        - ${action.set_ollama_data}

  sos_setup:
    ollama_setup: ${service.ollama.action.setup}
    local_data: ${service.local_data.action.setup}

  sos_install:
    pull_frontend_image:
      image_exists:
        tool: docker.image_exists
        context_map:
          - label: get image tag
            result: target
            data: namespace.frontend_image
        result_map:
          - label: set image exists result
            result: __TARGET__
            data: result

      pull_image:
        tool: docker.image_load
        params:
          source: registry
        context_map:
          - label: get image tag
            result: target
            data: namespace.frontend_image
        condition:
          - label: if image does not exist
            ctx_key: __TARGET__
            valid: False
            comparison: equal
            raise_exc: False

    ollama_data: ${action.set_ollama_data}
    download_models: ${action.download_models}

  sos_up:
    ollama_up: ${service.ollama.action.up}

    docker_compose:
      label: docker compose up
      tool: docker.compose_up
      context_map:
        - label: get profile
          result: profile
          data: meta.profile
        - label: generate compose file target
          result: file
          data:
            - target: system_path
              source: meta.system_path
            - target: compose_file
              source: namespace.runtime.compose_file
              default: docker-compose.yaml
          path_join: True
        - label: get project name
          result: project_name
          data: meta.system_name
        - label: generate env_map
          result: env_map
          data:
            - target: FRONTEND_IMAGE
              source: namespace.frontend_image
            - target: SYSTEM_NAME
              source: meta.system_name
            - target: SYSTEM_VERSION
              source: meta.system_version
            - target: OLLAMA_NETWORK
              source: service.ollama.namespace.network_name
            - target: LOCAL_DATA_PATH
              source: service.local_data.namespace.path
              default: /tmp/${meta.system_name}
            - target: FRONTEND_PORT
              source: namespace.runtime.port
            - target: OLLAMA_BASE_URL
              source: service.ollama.namespace.base_url

    service_address:
      tool: terminal.print_object
      params:
        horizontal_rule: True
      context_map:
        - label: generate service address
          result: obj
          data:
            - source: null
              target: preamble
              default: "OLLAMA-WEBUI RUNNING ON:"
            - source: null
              target: protocol
              default: http
            - source: namespace.runtime.host
              target: host
            - source: namespace.runtime.port
              target: port
          format_string: "{preamble} {protocol}://{host}:{port}"

  sos_down:
    compose:
      label: docker compose down
      tool: docker.compose_down
      context_map:
        - label: get project name
          result: project_name
          data: meta.system_name

    ollama_down: ${service.ollama.action.down}

  sos_status:
    root:
      label: root git status
      tool: git.repo_status
      context_map:
        - label: get system_path
          result: working_dir
          data: meta.system_path

  sos_clean:
    system_down: ${action.sos_down}

    frontend_image:
      image_exists:
        label: check image exists
        tool: docker.image_exists
        context_map:
          - label: get image tag
            result: target
            data: namespace.frontend_image
        result_map:
          - label: result to __TARGET__
            result: __TARGET__
            data: result
      delete_image:
        label: delete image if exists
        tool: docker.image_delete
        context_map:
          - label: get image tag
            result: target
            data: namespace.frontend_image
        condition:
          - label: if image exists
            ctx_key: __TARGET__
            valid: True
            is_inverse: False
            raise_exc: False

    # TODO
    # - fix permissions error => need to run ollama service container as non-root
    #local_data: ${service.local_data.action.clean}

  download_models:
    model_list:
      label: model_list break
      tool: context.runtime_break
      params:
        resolve:
          - label: model_list exists
            ctx_key: namespace.model_list
            valid: null
            is_inverse: True
            raise_exc: False
        qualifier: none
        info_break: NO MODEL_LIST PROVIDED
        info_pass: DOWNLOADING MODEL_LIST
    ollama_up: ${service.ollama.action.up}
    ollama_pull:
      label: pull required models
      tool: ollama.download_models
      context_map:
        - label: get required model list
          result: target
          data: namespace.model_list
    ollama_down: ${service.ollama.action.down}

  set_ollama_data:
    create_target:
      label: generate ollama data directory
      tool: context.ctx_set
      condition:
        - label: ollama_data exists
          ctx_key: namespace.ollama_data
          valid: null
          is_inverse: False
          raise_exc: False
      params:
        ctx_key: __TARGET__
        overwrite: True
      context_map:
        - label: generate create_child target
          result: obj
          data:
            - target: folder
              source: null
              default: ollama_data
            - target: name
              source: null
              default: ollama_data
      callbacks:
        - ${service.local_data.action.create_child}
        - label: set ollama service data directory
          tool: context.ctx_set
          params:
            ctx_key: service.ollama.namespace.model_dir
            overwrite: True
          context_map:
            - label: get model_dir
              result: obj
              data: service.local_data.namespace.child_map.ollama_data
        - label: generate model_dir break
          tool: context.runtime_break
          context_map:
            - label: generate info_break
              result: info_break
              data:
                - target: model_dir
                  source: service.ollama.namespace.model_dir
              format_string: "GENERATED OLLAMA MODEL DIR: {model_dir}"
    provided_target:
      label: map ollama data directory
      tool: context.ctx_set
      condition:
        - label: ollama_data exists
          ctx_key: namespace.ollama_data
          valid: null
          is_inverse: True
          raise_exc: False
      params:
        ctx_key: service.ollama.namespace.model_dir
        overwrite: True
      context_map:
        - label: get ollama data_dir
          result: obj
          data: namespace.ollama_data
      callbacks:
        - label: generate model_dir break
          tool: context.runtime_break
          context_map:
            - label: generate info_break
              result: info_break
              data:
                - target: model_dir
                  source: service.ollama.namespace.model_dir
              format_string: "PROVIDED OLLAMA MODEL DIR: {model_dir}"

hook:
  on_context_load:
    ollama_plugin: ${service.ollama.action.plugin}
